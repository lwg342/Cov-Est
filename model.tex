Our goal is to estimate \(\Sigma =\variance(y)\) where \(y\) is a \(p \times 1\) random vector, say asset returns. We collect the \(T\) observations into a matrix \(Y:p\times T\). Sample covariance estimate \(\hat{\Sigma} = \frac{1}{T} (Y - \bar{Y}\mathbf{1})(Y - \bar{Y}\mathbf{1})'\) is problematic when \(p\) is not small relative to \(T\). Popular estimation strategies include factor model, shrinkage, thresholding, banding, tapering, etc.

If in addition to the observation of \(Y\), we observe a network \(G\) among the firms, where \(G_{ij}\) either takes value \(0,1\) or a score in \([0,1]\), with higher \(G_{ij}\) implying that it's more ``likely'' that the returns of firm \(i,j\) are correlated. We show that this auxiliary network can be used to improve the estimation the covariance matrix \(\Sigma\). Examples of such network include \cite{hoberg2016TextBasedNetwork}, who identifies a product similarity network from financial reports that has been shown to be more accurate than industry block diagonal matrix. As linked firms are potentially subject to similar demand shock, we have reason to believe that \(G\) contains valuable information about the comovement among the returns.
\cite{israelsen2016does} and \cite{kaustia2020CommonAnalysts} both find that companies covered by the same analysts show similarities in many unobserved dimensions, and this analyst-based network could explain excess co-movement on top of common factors. With the development of machine learning techniques such as textual analysis, we are better at acquiring information from big data. Granular linkage information among firms that used to be notoriously hard to get due to its proprietary properties,  now are becoming available to researchers. The question is, how to use those auxiliary network information to  better estimate the co-movement between assets?

% For example, such \(G\) information could come from textual analysis, that has become more and more popular in finance, for example (\cite{fan2021HowMuch}). 

This paper aims to provide ways to extract the information contained in the auxiliary \(G\) matrix to help estimate the covariance \(\Sigma\). We consider an \textit{Adaptive Correlation Thresholding} method, where we apply thresholding to the correlation matrix, with the threshold level depending on network information. More specifically, suppose we observe \(Y_{t}\) for \(t = 1 ,\dots, T\), the procedure is 
% \begin{enumerate}
    % \item 
    % \item Guided Linear Shrinkage: we adopt the linear shrinkage method, where the shrainkge targets is chosen based on the network information. This is different from previous practice of shrinking to the identity or equicorrelation matrix. 
% \end{enumerate}

\begin{enumerate}
    \item Estimate the sample covariance estimate \(\hat{\Sigma}\), and the sample correlation matrix \(\hat{R}\). 
    \item Apply the generalized thresholding function \(h(r_{ij},\tau_{ij})\) to the off-diagonal elements of \(\hat{R} = (\hat{r}_{ij})\), as in  \cite{rothman2009GeneralizedThresholding}. The novelty is now we allow the threshold \(\tau_{ij}\) to vary across elements and to depend on the network information.  Specifications we have considered for the threshold \(\tau\) are 
    \begin{itemize}
        \item Simple linear model 
            \begin{equation*}
                \tau(G_{ij}) = a + bG_{ij}
            \end{equation*}
        \item 
        The probit model
            \begin{equation*}
            \tau_{ij} = \tau\pqty{G_{ij}} = \Phi\pqty{a + b \abs{G_{ij}}}
            \end{equation*}
    \end{itemize}
    \item Estimate the unknown parameters in the \(\tau\) function by cross validation, as in \cite{bickel2008CovarianceRegularization}, \cite{cai2011AdaptiveThresholding}, where we randomly split the sample \(V\) times, for each \(v\), compute the new estimator \(\hat{\Sigma}^{1,v}_{G}\) with the first subsample, and sample covariance \(\hat{\Sigma}^{2,v}\) and the criterion is 
    \begin{equation*}
        L(a, b) = \frac{1}{V} \sum_{v}^{V} \norm{\hat{\Sigma}^{1,v}_{G} - \hat{\Sigma}^{2,v}}^{2}_{F}
    \end{equation*}
    we find \(a,b\) that minimise this criterion. 
    
    \item Then with the estimates of \(a,b\), we can estimate \(\Sigma\) on the test sample. 
\end{enumerate}

%For the second method we consider, we construct a linear shrinkage target based on the hard-thresholded version of \(\hat{\Sigma}\), call it \(\hat{\Sigma}_{H} = [\hat{\Sigma}_{ij} \mathbf{1}\Bqty{G_{ij} > 0}]\). And apply linear shrinkage with the target. 

There are several advantages of using network guided method:
\begin{enumerate}
    \item The main advantage is that we are combining economically meaningful network with market-based performance data. Comparing to purely data-driven thresholding or shrinkage methods, the method utilizes valuable information embedded in external network data, which provides more robustness and efficiency. aif our auxiliary network contains the ``real'' links from the network. The relationship identified will be more stable over time than the relationship identified from return data alone. 
    \item This method is very flexible and extensible. Although in our current analysis we only use one of the existing networks as our proxy for $G$, you are free to include many candidate networks in the \(\tau\). You may want want to include characteristics-based distances, as it has been documented that companies with similar characteristics exhibit additional co-movement on top of common risk factors (see \cite{fernandez2011spatial} for example). It also provides a way to discern which set of information is relevant based an estimate of the coefficients \(a,b\) in the thresholding level. 
    \item The networks may provide industry-level comovement that is potentially related to the ``weak factors'' components, which we intend to investigate. 
\end{enumerate}

% There are some challanges and questions as well:
% \begin{enumerate}
    % \item Since we don't observe the true \(\Sigma\), we wonder if there are some good criteria for evaluating the performance of our \(\hat{\Sigma}\). So far I am following \cite{ledoit2004HoneyShrunk} and \cite{ledoit2017NonlinearShrinkage}. But unlike the linear and nonlinear shrinkage method, our estimator is not designed for optimal performance under the Frobenius norm. 
    % \item We don't have a quality measure of the network matrix or a model for how the \(G_{ij}\) are generated. 
    % \item In simulation studies, the optimization takes some time, but the result is not yet very robust(some times it outperforms all competitors, but sometimes the optimization will be very   off-target).
% \end{enumerate}
